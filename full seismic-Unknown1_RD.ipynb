{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d150ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import segyio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.signal import hilbert\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold,GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4c7ab47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def instantaneous_frequency(seismic_data, sample_rate=4):\n",
    "    # Compute the analytic signal using Hilbert Transform\n",
    "    analytic_signal = hilbert(seismic_data)\n",
    "    \n",
    "    # Compute the instantaneous phase (angle of analytic signal)\n",
    "    instantaneous_phase = np.angle(analytic_signal)\n",
    "    \n",
    " \n",
    "    # compute the derivative and multiply by the sample rate (in Hz)\n",
    "    phase_derivative = np.diff(instantaneous_phase)  # First derivative of phase\n",
    "    fs = 1000 / sample_rate  # Convert sample rate to Hz (1/sample_rate in seconds)\n",
    "    instantaneous_freq = np.abs(phase_derivative) * fs / (2 * np.pi)  # Frequency in Hz\n",
    "    \n",
    "    return instantaneous_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6466eddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 6.0120\n"
     ]
    }
   ],
   "source": [
    "# Folder path\n",
    "folder_path = r'C:\\Users\\sushila\\Desktop\\Hackathon'\n",
    "\n",
    "# Use glob to find all files that contain 'TEMP.csv' in the name\n",
    "files = glob.glob(os.path.join(folder_path, '*RT.csv'))\n",
    "\n",
    "# List to store data from each file\n",
    "data_frames = []\n",
    "\n",
    "# Loop through each file and read it\n",
    "for file in files:\n",
    "    # Extract the part of the filename before 'TEMP'\n",
    "    file_name = os.path.basename(file)\n",
    "    file_prefix = file_name.split('_RT')[0]\n",
    "    \n",
    "    # Read the CSV file into a DataFrame\n",
    "    df = pd.read_csv(file)\n",
    "    \n",
    "    # Add a new column with the file prefix\n",
    "    df['holeid'] = file_prefix\n",
    "    \n",
    "    # Append the DataFrame to the list\n",
    "    data_frames.append(df)\n",
    "\n",
    "# If you want to combine all the data into a single DataFrame\n",
    "combined_data = pd.concat(data_frames, ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "#Roundoff the depth so that it can be easily compared with depth from another dataframe\n",
    "combined_data['Depth'] = combined_data['Depth'].round(1)\n",
    "# Filter the dataset to include only the rows where holeid is either 'Lončarica-1' or 'Rezovačke Krčevine-1'\n",
    "data_unknown1 = combined_data[combined_data['holeid'].isin(['LONCCARICA_1' ])]\n",
    "\n",
    "X = data_unknown1[['Time (s)','Amplitude','Instantaneous_freq', 'Amplitude_abs','Amplitude_roll_mean','Amplitude_energy']]\n",
    "\n",
    "\n",
    "# Target variable is 'PHI'\n",
    "y = data_unknown1['RD']\n",
    "\n",
    "\n",
    "# Train-test split (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "def grid_search(clf, param_grid, X_train, y_train):\n",
    "    \"\"\"\n",
    "    Fits a classifier to its training data and prints its ROC AUC score.\n",
    "    \n",
    "    INPUT:\n",
    "    - clf (classifier): classifier to fit\n",
    "    - param_grid (dict): classifier parameters used with GridSearchCV\n",
    "    - X_train (DataFrame): training input\n",
    "    - y_train (DataFrame): training output\n",
    "            \n",
    "    OUTPUT:\n",
    "    - classifier: input classifier fitted to the training data\n",
    "    \"\"\"\n",
    "    # cv uses StratifiedKFold\n",
    "    # scoring r2 as parameter\n",
    "    grid = GridSearchCV(estimator=clf, \n",
    "                        param_grid=param_grid, \n",
    "                        scoring='r2', \n",
    "                        cv=5)\n",
    "    grid.fit(X_train, y_train)\n",
    "#     print(grid.best_score_)\n",
    "    \n",
    "    return grid.best_estimator_\n",
    "\n",
    "\n",
    "RF = RandomForestRegressor(n_estimators=100, random_state=100,n_jobs=-1)\n",
    "RF_best1 = grid_search(RF, {}, X_train = X_train, y_train=y_train)\n",
    "y_pred1=RF_best1.predict(X_test)\n",
    "\n",
    "rmse = np.sqrt(np.mean((y_pred1 - y_test)**2))\n",
    "print(f\"Test RMSE: {rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c7c02c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 7.1126\n"
     ]
    }
   ],
   "source": [
    "# Folder path\n",
    "folder_path = r'C:\\Users\\sushila\\Desktop\\Hackathon'\n",
    "\n",
    "# Use glob to find all files that contain 'TEMP.csv' in the name\n",
    "files = glob.glob(os.path.join(folder_path, '*RT.csv'))\n",
    "\n",
    "# List to store data from each file\n",
    "data_frames = []\n",
    "\n",
    "# Loop through each file and read it\n",
    "for file in files:\n",
    "    # Extract the part of the filename before 'TEMP'\n",
    "    file_name = os.path.basename(file)\n",
    "    file_prefix = file_name.split('_RT')[0]\n",
    "    \n",
    "    # Read the CSV file into a DataFrame\n",
    "    df = pd.read_csv(file)\n",
    "    \n",
    "    # Add a new column with the file prefix\n",
    "    df['holeid'] = file_prefix\n",
    "    \n",
    "    # Append the DataFrame to the list\n",
    "    data_frames.append(df)\n",
    "\n",
    "# If you want to combine all the data into a single DataFrame\n",
    "combined_data = pd.concat(data_frames, ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "#Roundoff the depth so that it can be easily compared with depth from another dataframe\n",
    "combined_data['Depth'] = combined_data['Depth'].round(1)\n",
    "# Filter the dataset to include only the rows where holeid is either 'Lončarica-1' or 'Rezovačke Krčevine-1'\n",
    "data_unknown1 = combined_data[combined_data['holeid'].isin(['VIROVITICA_1' ])]\n",
    "\n",
    "X = data_unknown1[['Time (s)','Amplitude','Instantaneous_freq', 'Amplitude_abs','Amplitude_roll_mean','Amplitude_energy']]\n",
    "\n",
    "\n",
    "# Target variable is 'PHI'\n",
    "y = data_unknown1['RD']\n",
    "\n",
    "\n",
    "# Train-test split (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "def grid_search(clf, param_grid, X_train, y_train):\n",
    "    \"\"\"\n",
    "    Fits a classifier to its training data and prints its ROC AUC score.\n",
    "    \n",
    "    INPUT:\n",
    "    - clf (classifier): classifier to fit\n",
    "    - param_grid (dict): classifier parameters used with GridSearchCV\n",
    "    - X_train (DataFrame): training input\n",
    "    - y_train (DataFrame): training output\n",
    "            \n",
    "    OUTPUT:\n",
    "    - classifier: input classifier fitted to the training data\n",
    "    \"\"\"\n",
    "    # cv uses StratifiedKFold\n",
    "    # scoring r2 as parameter\n",
    "    grid = GridSearchCV(estimator=clf, \n",
    "                        param_grid=param_grid, \n",
    "                        scoring='r2', \n",
    "                        cv=5)\n",
    "    grid.fit(X_train, y_train)\n",
    "#     print(grid.best_score_)\n",
    "    \n",
    "    return grid.best_estimator_\n",
    "\n",
    "\n",
    "RF = RandomForestRegressor(n_estimators=100, random_state=100,n_jobs=-1)\n",
    "RF_best2 = grid_search(RF, {}, X_train = X_train, y_train=y_train)\n",
    "y_pred2=RF_best2.predict(X_test)\n",
    "\n",
    "rmse = np.sqrt(np.mean((y_pred2 - y_test)**2))\n",
    "print(f\"Test RMSE: {rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5566d7d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of traces: 744\n",
      "All predictions saved to all_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "import segyio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize the empty DataFrame to store all predictions\n",
    "all_predictions = pd.DataFrame(columns=['Trace_Index', 'Time (s)', 'RT'])\n",
    "\n",
    "# Path to SEG-Y file\n",
    "path = 'C://Users/sushila/Desktop/Data_shared_with Participants/Data/2D Seismic/Unkown-1/LO-6-86_01._KNGD export.sgy'\n",
    "\n",
    "# Open SEG-Y file and process all traces\n",
    "with segyio.open(path, \"r\", strict=False) as f:\n",
    "    f.mmap()  # Memory map the file for efficient access\n",
    "    \n",
    "    # Get the number of traces\n",
    "    num_traces = len(f.trace)\n",
    "    print(f\"Number of traces: {num_traces}\")\n",
    "\n",
    "    # Loop through each trace and process them individually\n",
    "    for trace_index in range(num_traces):\n",
    "        trace_data = f.trace[trace_index]  # Get the trace data\n",
    "        \n",
    "        # Retrieve sampling interval (in ms) and calculate time axis\n",
    "        dt = segyio.dt(f) / 1000  # Convert to milliseconds (time in ms)\n",
    "        time_axis = np.arange(len(trace_data)) * dt  # Time axis for the trace (in ms)\n",
    "        time_axis = time_axis / 1000\n",
    "\n",
    "        # Create DataFrame for the trace\n",
    "        df_trace = pd.DataFrame({\n",
    "            \"Time (s)\": time_axis,  # Time in milliseconds\n",
    "            \"Amplitude\": trace_data\n",
    "        })\n",
    "        \n",
    "        # Calculate instantaneous frequency\n",
    "        instantaneous_freq = instantaneous_frequency(trace_data, sample_rate=4) \n",
    "\n",
    "        # Add NaN to the start to align the length\n",
    "        instantaneous_freq_full = np.concatenate(([np.nan], instantaneous_freq))\n",
    "        df_trace['Instantaneous_freq'] = instantaneous_freq_full\n",
    "        # Feature engineering for resampled data\n",
    "        df_trace['Amplitude_abs'] = df_trace['Amplitude'].abs()\n",
    "        df_trace['Amplitude_roll_mean'] = df_trace['Amplitude'].rolling(window=5, min_periods=1).mean()\n",
    "        df_trace['Amplitude_energy'] = df_trace['Amplitude'] ** 2\n",
    "        # Resample and feature engineering \n",
    "        df_resampled = df_trace  # Simplified for clarity\n",
    "        # Drop rows with NaN values in the features\n",
    "        df_resampled_clean = df_resampled.dropna(subset=['Time (s)', 'Amplitude', 'Instantaneous_freq', 'Amplitude_abs', 'Amplitude_roll_mean', 'Amplitude_energy']).copy()\n",
    "        X_scaled = scaler.transform(df_resampled_clean)\n",
    "        y_pred_trace1 = RF_best1.predict(X_scaled)\n",
    "        y_pred_trace2 = RF_best2.predict(X_scaled)\n",
    "        y_pred_trace = (y_pred_trace1 + y_pred_trace2)/2\n",
    "        \n",
    "        # Store predicted TEMP in the DataFrame\n",
    "        df_resampled_clean['RT'] = y_pred_trace\n",
    "        \n",
    "        # Filter based on a time condition (e.g., <= 1.2 seconds)\n",
    "        df_resampled_clean = df_resampled_clean[df_resampled_clean['Time (s)'] <= 1.2].copy()\n",
    "\n",
    "        # Check if the DataFrame is empty after filtering\n",
    "        if df_resampled_clean.empty:\n",
    "            print(f\"Trace {trace_index}: No data after filtering. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Create a new time range with step size of 0.001 seconds for resampling\n",
    "        new_time = np.arange(\n",
    "            df_resampled_clean['Time (s)'].min(),\n",
    "            df_resampled_clean['Time (s)'].max(),\n",
    "            0.001\n",
    "        )\n",
    "\n",
    "        # Interpolate TEMP values to match the new time range\n",
    "        new_temp = np.interp(new_time, df_resampled_clean['Time (s)'], df_resampled_clean['RT'])\n",
    "\n",
    "        # Append predictions to the master DataFrame\n",
    "        temp_df = pd.DataFrame({\n",
    "            'Trace_Index': trace_index,\n",
    "            'Time (s)': new_time,\n",
    "            'RT': new_temp\n",
    "        })\n",
    "        all_predictions = pd.concat([all_predictions, temp_df], ignore_index=True)\n",
    "\n",
    "\n",
    "# Save all predictions to a CSV file\n",
    "all_predictions.to_csv(\"all_predictions_RT.csv\", index=False)\n",
    "print(\"All predictions saved to all_predictions.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "986c1758",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Trace_Index</th>\n",
       "      <th>Time (s)</th>\n",
       "      <th>RT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.004</td>\n",
       "      <td>16.907875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.005</td>\n",
       "      <td>16.907875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.006</td>\n",
       "      <td>16.907875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.007</td>\n",
       "      <td>16.907875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.008</td>\n",
       "      <td>16.907875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889819</th>\n",
       "      <td>743</td>\n",
       "      <td>1.195</td>\n",
       "      <td>22.345053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889820</th>\n",
       "      <td>743</td>\n",
       "      <td>1.196</td>\n",
       "      <td>22.364261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889821</th>\n",
       "      <td>743</td>\n",
       "      <td>1.197</td>\n",
       "      <td>22.312574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889822</th>\n",
       "      <td>743</td>\n",
       "      <td>1.198</td>\n",
       "      <td>22.260887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889823</th>\n",
       "      <td>743</td>\n",
       "      <td>1.199</td>\n",
       "      <td>22.209200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>889824 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Trace_Index  Time (s)         RT\n",
       "0                0     0.004  16.907875\n",
       "1                0     0.005  16.907875\n",
       "2                0     0.006  16.907875\n",
       "3                0     0.007  16.907875\n",
       "4                0     0.008  16.907875\n",
       "...            ...       ...        ...\n",
       "889819         743     1.195  22.345053\n",
       "889820         743     1.196  22.364261\n",
       "889821         743     1.197  22.312574\n",
       "889822         743     1.198  22.260887\n",
       "889823         743     1.199  22.209200\n",
       "\n",
       "[889824 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cadc56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
